{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "lambda = 0;\ttrain=0.732843137255; validate=0.720759338641; test=0.77709736681\n",
      "lambda = 0.01;\ttrain=0.732230392157; validate=0.721984078383; test=0.780159216167\n",
      "lambda = 1.0;\ttrain=0.726715686275; validate=0.704225352113; test=0.766074709124\n",
      "lambda = 100.0;\ttrain=0.658700980392; validate=0.630128597673; test=0.696876913656\n",
      "lambda = 0;\ttrain=0.75; validate=0.757501530925; test=0.738518064911\n",
      "lambda = 0.01;\ttrain=0.748774509804; validate=0.758113900796; test=0.739742804654\n",
      "lambda = 1.0;\ttrain=0.731617647059; validate=0.756276791182; test=0.732394366197\n",
      "lambda = 100.0;\ttrain=0.66237745098; validate=0.681567666871; test=0.680342927128\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from math import exp\n",
    "from math import log\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "print \"Reading data...\"\n",
    "dataFile = open(\"winequality-white.csv\")\n",
    "header = dataFile.readline()\n",
    "fields = [\"constant\"] + header.strip().replace('\"','').split(';')\n",
    "featureNames = fields[:-1]\n",
    "labelName = fields[-1]\n",
    "lines = [[1.0] + [float(x) for x in l.split(';')] for l in dataFile]\n",
    "\n",
    "X = [l[:-1] for l in lines]\n",
    "y = [l[-1] > 5 for l in lines]\n",
    "\n",
    "def inner(x,y):\n",
    "  return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1.0 / (1 + exp(-x))\n",
    "\n",
    "def f(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    if not y[i]:\n",
    "      loglikelihood -= logit\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print \"ll =\", loglikelihood\n",
    "  return -loglikelihood\n",
    "\n",
    "def fprime(theta, X, y, lam):\n",
    "  dl = [0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    for k in range(len(theta)):\n",
    "      dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "      if not y[i]:\n",
    "        dl[k] -= X[i][k]\n",
    "  for k in range(len(theta)):\n",
    "    dl[k] -= lam*2*theta[k]\n",
    "  return numpy.array([-x for x in dl])\n",
    "\n",
    "X_train = X[:int(len(X)/3)]\n",
    "y_train = y[:int(len(y)/3)]\n",
    "X_validate = X[int(len(X)/3):int(2*len(X)/3)]\n",
    "y_validate = y[int(len(y)/3):int(2*len(y)/3)]\n",
    "X_test = X[int(2*len(X)/3):]\n",
    "y_test = y[int(2*len(X)/3):]\n",
    "\n",
    "def train(lam):\n",
    "  theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "  return theta\n",
    "\n",
    "# End of setup code\n",
    "\n",
    "# 1\n",
    "def performance(theta):\n",
    "  scores_train = [inner(theta,x) for x in X_train]\n",
    "  scores_validate = [inner(theta,x) for x in X_validate]\n",
    "  scores_test = [inner(theta,x) for x in X_test]\n",
    "\n",
    "  predictions_train = [s > 0 for s in scores_train]\n",
    "  predictions_validate = [s > 0 for s in scores_validate]\n",
    "  predictions_test = [s > 0 for s in scores_test]\n",
    "\n",
    "  correct_train = [(a==b) for (a,b) in zip(predictions_train,y_train)]\n",
    "  correct_validate = [(a==b) for (a,b) in zip(predictions_validate,y_validate)]\n",
    "  correct_test = [(a==b) for (a,b) in zip(predictions_test,y_test)]\n",
    "  \n",
    "  acc_train = sum(correct_train) * 1.0 / len(correct_train)\n",
    "  acc_validate = sum(correct_validate) * 1.0 / len(correct_validate)\n",
    "  acc_test = sum(correct_test) * 1.0 / len(correct_test)\n",
    "  return acc_train, acc_validate, acc_test\n",
    "\n",
    "for lam in [0, 0.01, 1.0, 100.0]:\n",
    "  theta = train(lam)\n",
    "  acc_train, acc_validate, acc_test = performance(theta)\n",
    "  print(\"lambda = \" + str(lam) + \";\\ttrain=\" + str(acc_train) + \"; validate=\" + str(acc_validate) + \"; test=\" + str(acc_test))\n",
    "\n",
    "# 2\n",
    "shuffled = lines[:]\n",
    "random.shuffle(shuffled)\n",
    "\n",
    "X = [l[:-1] for l in shuffled]\n",
    "y = [l[-1] > 5 for l in shuffled]\n",
    "\n",
    "X_train = X[:int(len(X)/3)]\n",
    "y_train = y[:int(len(y)/3)]\n",
    "X_validate = X[int(len(X)/3):int(2*len(X)/3)]\n",
    "y_validate = y[int(len(y)/3):int(2*len(y)/3)]\n",
    "X_test = X[int(2*len(X)/3):]\n",
    "y_test = y[int(2*len(X)/3):]\n",
    "\n",
    "for lam in [0, 0.01, 1.0, 100.0]:\n",
    "  theta = train(lam)\n",
    "  acc_train, acc_validate, acc_test = performance(theta)\n",
    "  print(\"lambda = \" + str(lam) + \";\\ttrain=\" + str(acc_train) + \"; validate=\" + str(acc_validate) + \"; test=\" + str(acc_test))\n",
    "\n",
    "X = [l[:-1] for l in lines]\n",
    "y = [l[-1] > 5 for l in lines]\n",
    "\n",
    "X_train = X[:int(len(X)/3)]\n",
    "y_train = y[:int(len(y)/3)]\n",
    "X_validate = X[int(len(X)/3):int(2*len(X)/3)]\n",
    "y_validate = y[int(len(y)/3):int(2*len(y)/3)]\n",
    "X_test = X[int(2*len(X)/3):]\n",
    "y_test = y[int(2*len(X)/3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives 1129.0\n",
      "True negatives 145.0\n",
      "False positives 321.0\n",
      "False negatives 38.0\n",
      "Balanced error rate 0.178304665865\n",
      "Query size = 10 \tPrecision = 1.0 \tRecall = 0.00856898029135\n",
      "Query size = 500 \tPrecision = 0.956 \tRecall = 0.409597257926\n",
      "Query size = 1000 \tPrecision = 0.864 \tRecall = 0.740359897172\n",
      "3675818.61688\n",
      "[[ -3.23636346e-04   1.42201752e-04   3.17030713e-04   5.36390435e-02\n",
      "    9.30284526e-05   2.54030965e-01   9.65655009e-01   3.19990241e-05\n",
      "   -2.95831396e-04   3.84043646e-04  -1.00526693e-02]\n",
      " [ -7.57985623e-03  -1.66366340e-03   1.04742899e-03   5.21677266e-02\n",
      "    4.49425600e-05   9.65020304e-01  -2.56793964e-01   7.90089050e-06\n",
      "    5.24900596e-04  -1.09699394e-03  -2.89827657e-03]\n",
      " [  1.82124420e-02   2.54680710e-03   3.31838657e-03   9.93221259e-01\n",
      "   -1.51888372e-04  -6.42297821e-02  -3.91682592e-02   4.30929482e-04\n",
      "   -6.93199060e-03  -2.85216045e-03  -8.62920933e-02]\n",
      " [  1.56811999e-01   3.28220652e-03   1.66866136e-02   8.28549640e-02\n",
      "   -6.91822288e-03   1.13029682e-03   5.39110108e-03  -9.49080503e-04\n",
      "    2.68027305e-03   1.30498102e-03   9.83955205e-01]\n",
      " [  9.81360642e-01  -1.45890108e-02   5.92643662e-02  -3.17546064e-02\n",
      "    5.07483182e-04   8.43759364e-03  -1.77578042e-03   6.03725221e-04\n",
      "   -9.05011239e-02  -9.35630845e-03  -1.54417839e-01]\n",
      " [  7.76578401e-02  -2.37665885e-01   2.23406619e-02   5.04113878e-03\n",
      "   -1.43564098e-02  -2.14210997e-04  -2.22913844e-04   3.36617054e-03\n",
      "    8.77254205e-01   4.08570175e-01  -1.54145486e-02]\n",
      " [ -7.36289612e-02  -2.61563804e-01   9.43067566e-01  -2.14514264e-03\n",
      "    1.19104298e-02  -1.68808905e-03   1.42294158e-04  -1.17203197e-04\n",
      "   -1.45895558e-01   1.23868963e-01  -2.88797236e-03]\n",
      " [ -1.37617196e-02   2.11129619e-01  -1.16514121e-01   5.30670319e-04\n",
      "    1.05181628e-02   1.36446528e-03  -8.21179429e-04   3.09221855e-04\n",
      "   -3.58358431e-01   9.01728510e-01   3.27758247e-03]\n",
      " [  1.74575775e-02   9.10890084e-01   3.04081497e-01  -2.89763923e-03\n",
      "    2.34615054e-02   1.17406025e-03  -3.85957239e-04   1.23176271e-03\n",
      "    2.68927937e-01  -6.70756658e-02  -1.12101920e-02]\n",
      " [  2.31513441e-03  -2.38717789e-02  -1.67445603e-02   8.92206499e-04\n",
      "    9.99462734e-01  -9.81109101e-05  -3.32812875e-05   4.14235255e-03\n",
      "    1.18483756e-02  -3.51543098e-03   6.92344110e-03]\n",
      " [  7.48312160e-04   3.08204153e-04   2.55232500e-04   3.49846801e-04\n",
      "    4.12943179e-03  -6.96565372e-06   4.16951216e-06  -9.99984215e-01\n",
      "    3.17948604e-03   1.53436134e-03  -1.10029138e-03]]\n",
      "[  1.76611818e+02   7.73266745e-01   1.03590798e+01   1.24531982e+01\n",
      "   4.67344579e+00   3.22698912e+00  -7.49640232e-01  -7.88158703e-01\n",
      "   1.08509039e+00   1.56140166e-01  -9.87201543e-01]\n",
      "64121.78222\n",
      "64121.78222\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "lam = 0.01\n",
    "theta = train(lam)\n",
    "\n",
    "def ber(theta):\n",
    "    scores_test = [inner(theta,x) for x in X_test]\n",
    "    predictions_test = [s > 0 for s in scores_test]\n",
    "    \n",
    "    tp, tn, fp, fn = 0.0, 0.0, 0.0, 0.0\n",
    "    for prediction, label in zip(predictions_test, y_test):\n",
    "        if prediction and label:\n",
    "            tp += 1\n",
    "        elif not prediction and not label:\n",
    "            tn += 1\n",
    "        elif prediction and not label:\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    fpr, fnr = fp / (tp + fn), fn / (tn + fp)\n",
    "#     TP = float(sum([(a and b) for (a,b) in zip(predictions_test, y_test)]))\n",
    "#     TN = float(sum([(not a and not b) for (a,b) in zip(predictions_test, y_test)]))\n",
    "#     FP = float(sum([(a and not b) for (a,b) in zip(predictions_test, y_test)]))\n",
    "#     FN = float(sum([(not a and b) for (a,b) in zip(predictions_test, y_test)]))\n",
    "\n",
    "    return tp, tn, fp, fn, (fpr + fnr) / 2\n",
    "#     fpr, fnr = FP / (TP + FN), FN / (TN + FP)\n",
    "#     return TP, TN, FP, FN, (fpr + fnr) / 2\n",
    "\n",
    "true_pos, true_neg, false_pos, false_neg, balanced_error_rate = ber(theta)\n",
    "print \"True positives\", true_pos\n",
    "print \"True negatives\", true_neg\n",
    "print \"False positives\", false_pos\n",
    "print \"False negatives\", false_neg\n",
    "print \"Balanced error rate\", balanced_error_rate\n",
    "\n",
    "# 4\n",
    "def rank(theta, n):\n",
    "    scores_test = [inner(theta,x) for x in X_test]\n",
    "    ranking = sorted(zip(scores_test, y_test), reverse=True)\n",
    "    relevant_retrieved = len([y for _, y in ranking[:n] if y])\n",
    "    relevant = len([y for _, y in ranking if y])\n",
    "    return float(relevant_retrieved) / n, float(relevant_retrieved) / relevant\n",
    "\n",
    "for predictions in [10, 500, 1000]:\n",
    "    precision, recall = rank(theta, predictions)\n",
    "    print \"Query size =\", predictions, \"\\tPrecision =\", precision, \"\\tRecall =\", recall\n",
    "\n",
    "# 5\n",
    "unbiased = numpy.array([row[1:] for row in X_train])\n",
    "mean = numpy.mean(unbiased, axis=0)\n",
    "compression = numpy.tile(mean, (len(X_train), 1))\n",
    "diff = compression - unbiased\n",
    "print sum([numpy.dot(d, d) for d in diff])\n",
    "\n",
    "# 6\n",
    "pca = PCA()\n",
    "pca.fit(unbiased)\n",
    "print pca.components_\n",
    "\n",
    "# 7\n",
    "print numpy.dot(unbiased[0], pca.components_.T)\n",
    "\n",
    "# 8\n",
    "# Explicitly compress and uncompress the data\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(unbiased)\n",
    "compressed = numpy.dot(unbiased, pca.components_.T)\n",
    "uncompressed = numpy.dot(compressed, pca.components_)\n",
    "diff = unbiased - uncompressed\n",
    "print sum([numpy.dot(d, d) for d in diff])\n",
    "\n",
    "# Using the definition of reconstruction error\n",
    "mean = numpy.tile(numpy.mean(unbiased, axis=0), (len(X_train), 1))\n",
    "diff = unbiased - mean\n",
    "pca = PCA()\n",
    "pca.fit(unbiased)\n",
    "diff_transform = numpy.dot(unbiased, pca.components_[4:].T)\n",
    "print sum([numpy.dot(d, d) for d in diff_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.02365196,   0.28001532,   0.36526348, ...,   3.20603554,\n",
       "          0.4860723 ,  10.29007353],\n",
       "       [  7.02365196,   0.28001532,   0.36526348, ...,   3.20603554,\n",
       "          0.4860723 ,  10.29007353],\n",
       "       [  7.02365196,   0.28001532,   0.36526348, ...,   3.20603554,\n",
       "          0.4860723 ,  10.29007353],\n",
       "       ..., \n",
       "       [  7.02365196,   0.28001532,   0.36526348, ...,   3.20603554,\n",
       "          0.4860723 ,  10.29007353],\n",
       "       [  7.02365196,   0.28001532,   0.36526348, ...,   3.20603554,\n",
       "          0.4860723 ,  10.29007353],\n",
       "       [  7.02365196,   0.28001532,   0.36526348, ...,   3.20603554,\n",
       "          0.4860723 ,  10.29007353]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
